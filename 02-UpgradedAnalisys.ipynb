{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miglioriamo l'analisi\n",
    "====\n",
    "\n",
    "Miglioriamo l'analisi andando ad aggiungere \n",
    "\n",
    "* packed padded sequences\n",
    "* pre-trained word embeddings\n",
    "* una architettura RNN differente\n",
    "* bidirectional RNN\n",
    "* multi-layer RNN\n",
    "* regolarizzazione\n",
    "* un ottimizzatore differente\n",
    "\n",
    "\n",
    "Prima di eseguire qualsiasi altro passo andiamo ad impostare il seed per ottenere sempre gli stessi risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparare i dati\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a gestire le packed padded sequences, questo permette alla RNN di processare solo gli elementi non ```<pad>``` e per gli elementi ```<pad>``` andremo a caricare un tensore a 0. Per far questo dobbiamo dire alla RNN quanto la nostra sequenza è lunga. Per farlo basta impostare  ```include_lengths = True``` per il nostro campo TEXT. ora ```batch.text``` diventerà una tupla con il primo elemento la nostra sequenza e come secondo elemento la lunghezza della stessa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carichiamo il dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "andiamo a creare il dataset di validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora andiamo a caricare il word embeddings, pre-trained. Invece che avere il nostro strato di embeddings andiamo ad utilizzare una rappresentazione della parola pre-trained. Per farlo basta semplicemente dire che vettore vogliamo caricare e passarlo come argomento a build_vocab. TorchText scaricherà per noi i vettori e associerà ad ogni parola del dizionario il vettore corretto del nostro dizionario.\n",
    "\n",
    "Useremo il \"glove.6B.100d\". Glove è un algoritmo di vettorializzazione 6B sta ad indicare che i vettori sono stati addestrati su 6 Miliardi di token e hanno una dimensionalità di 100\n",
    "\n",
    "Per vedere gli altri vettori disponibili basta andare [qui](https://github.com/pytorch/text/blob/master/torchtext/vocab.py#L146).\n",
    "\n",
    "In questo spazio vettoriale le parole con un significato simile sono adiacenti, ad esempio \"terrible\", \"awful\", \"dreadful\" sono posizionate vicine. Questo è una buona cosa in quanto il modello non deve imparare queste relazioni da zero.\n",
    "\n",
    "**Attenzione** questi vettori hanno una dimensione di circa 862MB.\n",
    "\n",
    "Di default, TorchText inizializzerà le parole nel nostro vocabolario con un vettore a 0, noi non vogliamo questo andremo ad impostare il token UNK randomicamente impostando ```unk_init``` con ```torch.Tensor.normal_```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come prima, creiamo gli iteratori e piazziamo i tensori nella GPU se questa è disponibile. \n",
    "Unl'altra cosa per gestire le frasi con packed padded, tutti i tensori dentro al batch devono essere ordinati per la loro lunghezza. Questo viene gestito con il parametro ```sort_within_batch = True``` nell'iteratore. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#per getire il problema con win10 e i driver cuda commentare prima di fare la push\n",
    "#device = 'cpu'\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruzione del modello\n",
    "----\n",
    "\n",
    "Il modello che andremo a costruire è molto differente dal precedente\n",
    "\n",
    "Architettura LSTM\n",
    "----\n",
    "\n",
    "Perchè la rete LSTM è migliore di una RNN ? Le reti RNN soffrono del problema del vanishing gradient.\n",
    "Le reti LSTM superano questo problema con uno strato di memoria extra chiamato cell. **TODO: spiegare meglio**\n",
    "\n",
    "<img src=\"./images/lstm.png\" />\n",
    "\n",
    "Bidirectional RNN\n",
    "----\n",
    "\n",
    "Il concetto dietro una RNN Bidirezionale è semplice. Abbiamo una RNN che processa i tokens in un verso (forward) e una RNN che processa gli stessi nel verso opposto (backward).\n",
    "In pyhtorch i tensori hidden state e backward sono impilati uno sopra l'altro in un singolo tensore.\n",
    "\n",
    "<img src=\"./images/bidirectional.png\" />\n",
    "\n",
    "Multi Layer RNN\n",
    "----\n",
    "\n",
    "Le reti multilayer (dette anche deep RNN) sono un concetto abbastanza semplice.\n",
    "L'idea è che possiamo aggiungere una RNN addizionale sopra un'altra RNN. L'idea è che l'output (hidden state) della rete più in basso diventa l'input della rete più in alto. \n",
    "\n",
    "<img src=\"./images/multilayer.png\" />\n",
    "\n",
    "Regolarizzazione\n",
    "----\n",
    "\n",
    "Sebbene abbiamo aggiunto molti miglioramenti al modello, abbiamo anche aggiunto molti parametri addizionali.Esiste il rischio concreto di overfitting. Per combattere questo introduciamo la regolarizzazione nel nostro caso il dropout.\n",
    "\n",
    "Implementazione\n",
    "----\n",
    "\n",
    "Un'altro miglioramento al modello è che non andremo ad imparare dal token pad, andando esplicitamente a dire al modello che tali token sono irrilevanti per determinare il sentimento di una sequenza.\n",
    "\n",
    "La rete LSTM non ritorna semplicemente uno stato hidden ma bensi uno stato di output, e una tupla di stati hidden e cell.\n",
    "Lo stato finale della nostra LSTM, sia la componente forward che la componente backward vengono concatenate assieme (Sicchè l'ingresso della f.c. finale sarà il doppio dello hidden state).\n",
    "\n",
    "Per implementare la bidirezionalità e aggiungere layer addizionali vanno usati i parametri ```num_layers``` e ```bidirectional```.\n",
    "\n",
    "Per poter utilizzare il packed padded sequences, dobbiamo aggiungere il parametro ```text_lenght``` a forward. Prima di passare il tensore di embeddings alla RNN dobbiamo impacchettarlo (Pack), questo farà si che la nostra rete analizzerà solo i token non pad.\n",
    "\n",
    "La RNN restituisce packed_output e la tupla hidden e cell.\n",
    "\n",
    "Senza la gestione del pad l'ultimo stato si sarebbe riferito quasi sicurtamente al pad. Usiamo la funzione ```unpack``` per andare ad ottenere di nuovo il tensore alla dimensione originale. Gli elementi padding di output vengono riportati a 0, viene fatto se si utilizzare di nuovo la sequenza più avanti nel modello.\n",
    "\n",
    "Lo stato finale ha una dimensionalità **[num layers * num directions, batch size, hid dim]** e i layer sono ordinati in questo modo **[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]**.\n",
    "\n",
    "Se vogliamo il layer finale forward e backward dobbiamo prendere il penultimo e ultimo dalla prima dimensione e concatenarlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #dimensione del batch text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #ora output ha la dimensione originale\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concateno il layer  forward finale (hidden[-2,:,:]) e il backward (hidden[-1,:,:]) \n",
    "        #e gli applico il droput\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come prima ci andiamo a instanziare il modello impostando i nuovi parametri\n",
    "Dobbiamo impostare la dimensionalità di embedding uguale a quella dei vettori GloVe caricati prima.\n",
    "Andiamo anche ad estrarre l'indice del token  ```<pad> ```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a vedere il numero dei parametri del modello, notiamo che ora sono più del doppio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,810,857 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora dobbiamo copiare il pre-trained word embeddings dentro lo strato di embedding del modello.\n",
    "Controlliamo anche la dimensionalità di embedding che deve essere **[vocab size, embedding dim]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a copiare il tensore all'interno di embeddings.\n",
    "\n",
    "**NOTA** Questo deve essere fatto usando weight.data e non weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.1386,  0.1180,  0.3534,  ...,  0.1226,  0.5973, -0.1702],\n",
       "        [-0.0786,  0.0541, -0.0993,  ...,  0.2565, -0.1874, -0.4428],\n",
       "        [-0.3617,  0.6201,  0.1105,  ...,  0.2994, -0.5920,  1.0949]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo ad azzerare le posizioni di UNK e PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [-0.1386,  0.1180,  0.3534,  ...,  0.1226,  0.5973, -0.1702],\n",
      "        [-0.0786,  0.0541, -0.0993,  ...,  0.2565, -0.1874, -0.4428],\n",
      "        [-0.3617,  0.6201,  0.1105,  ...,  0.2994, -0.5920,  1.0949]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo ad impostare l'ottimizzatore e il criterio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train del modello\n",
    "----\n",
    "\n",
    "L'unico cambiamento che andiamo a fare è relativo all'ottimizzatore invece che **SDG** andiamo ad usare **ADAM** [qui](https://ruder.io/optimizing-gradient-descent/index.html) una guidina sui vari otimizzatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperiamo anche la funzione per calcolare l'accuratezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo la funzione di trainig per il nostro modello.\n",
    "\n",
    "Siccome abbiamo impostato include_lengths = True il nostro batch.text ora è una tupla con il primo elemento la lista dei vari token tradotti in numeri e il secondo elemento il numero di token nella prima lista. Separiamo il risultato in due variabili che passeremo al modello \n",
    "\n",
    "**Nota:** Ora stiamo usando il droput, dobbiamo assicurarci che sia abilitato quando andiamo a fare il train usando il metodo model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ora andiamo a fare la valutazione del modello model.eval() spegne il dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** con windows 10 ci sono problemi con le reti lstm ho disabilitato il train, per riabilitarlo commentare la prima riga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "#%%script false \n",
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ora andiamo a caricare il modello e valutiamo il risultato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.973 | Test Acc: 57.88%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut2-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input personalizzato\n",
    "----\n",
    "\n",
    "Possiamo utilizzare il modello per predire il sentimento su una nostra nuova frase.\n",
    "Usiamo il modello in evaluation mode. La nostra funzione predict_sentiment fa le seguenti attività:\n",
    "\n",
    "* imposta il modello in evaluation mode\n",
    "* tokenizza la frase in ingresso andando a spezzare la stringa in una lista di token\n",
    "* prende i vari token e li trasforma nella rappresentazione numerica andando a utilizzare il nostro vocabolario\n",
    "* legge la lunghezza della frase\n",
    "* converte la rappresentazione numerica in una lista di tensori\n",
    "* aggiungo una dimensione unsqueezeing per poter farla gestire a pytorch\n",
    "* converte la lunghezza in un tensore\n",
    "* spalmo il risultato del risultato del modello tra 0 e 1 tramite la funzione di sigmoid\n",
    "* converto il tensore in un intero tramire la funzione item()\n",
    "    \n",
    "\n",
    "Ora per le recensioni negative ci aspettiamo che il modello ritorni un valore vicino allo 0 e per le recensioni positive ci venga restituito un valore vicino a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "andiamo ad analizzare un esempio di recensione negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7027969360351562"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "un esempio di recensione positiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8941178917884827"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

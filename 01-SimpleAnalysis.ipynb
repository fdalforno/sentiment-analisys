{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisi semplice\n",
    "====\n",
    "\n",
    "In questo notebook iniziamo una analisi semplice per capire il funzionamento di PyTorch e TorchText useremo come riferimento il dataset imdb, non ci interessa il risultato finale ma solo capire il funzionamento \n",
    "\n",
    "Per prima cosa impostiamo il seed in modo da avere un esperimento riproducibile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparare i dati\n",
    "----\n",
    "\n",
    "Il concetto principale di TorchText è il Field, definisce come il processare il testo.\n",
    "Nel nostro Dataset abbiamo in ingresso una stringa con il commento da analizzare e l'etichetta che classifica il commento.\n",
    "Il parametro dell'oggetto Field ci indica come l'oggetto deve essere processato.\n",
    "Per comodità definiamo due tipi di campo:\n",
    "\n",
    " + TEXT che elabora la recensione\n",
    "\n",
    " + LABEL che elabora l'etichetta\n",
    "\n",
    "TEXT ha impostato come proprietà tokenize='spacy', se non viene passato nulla a questa proprietà la stringa verrà spezzata usando gli spazi.\n",
    "\n",
    "LABEL è impostato come LabelField una sottoclasse di Field usata per gestire le etichetto\n",
    "\n",
    "per maggiori info [link](https://github.com/pytorch/text/blob/master/torchtext/data/field.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a scaricare il dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlliamo quanti dati contiene e la divisione per classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n",
      "Number of training positive: 12500 negative 12500\n",
      "Number of testing positive: 12500 negative 12500\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "pos = 0\n",
    "neg = 0\n",
    "\n",
    "for row in train_data[:]:\n",
    "    label = row.label\n",
    "    if(label == 'pos'):\n",
    "        pos += 1\n",
    "    else:\n",
    "        neg +=1\n",
    "\n",
    "print(f'Number of training positive: {pos} negative {neg}')\n",
    "\n",
    "\n",
    "pos = 0\n",
    "neg = 0\n",
    "\n",
    "for row in test_data[:]:\n",
    "    label = row.label\n",
    "    if(label == 'pos'):\n",
    "        pos += 1\n",
    "    else:\n",
    "        neg +=1\n",
    "        \n",
    "print(f'Number of testing positive: {pos} negative {neg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vediamo come è fatto un esempio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se il dataset ha solo train e test possiamo usare la funzione ```.split()``` per dividere il dataset. Di default viene impostato un rapporto 70/30 che puo essere cambiato usando il parametro ```split_ratio```, possiamo anche passare il ```random_state``` per essere sicuri di ottenere sempre lo stesso risultato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobbiamo creare un vocabolario, dove ogni parola ha un numero corrispondente, indice (index).\n",
    "\n",
    "Questo perchè quasi tutti i modelli di machine learning operano su numeri.Verrà costruita una serie di vettori _one-hot_ \n",
    "uno per ogni parola.\n",
    "\n",
    "Un vettore _one-hot_ ha tutti gli elementi a 0 tranne una posizione impostata a 1 (index) la dimensionalità è il totale \n",
    "del numero di parole univoche del dizionario, comunemente denominata $V$ \n",
    "\n",
    "<img src=\"./images/one-hot.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora il numero di parole all'interno del dataset supera 100'000, questo significa che la dimensionalità di $V$ supera quella cifra.\n",
    "Questo può dare problemi con la memoria della GPU, andiamo dunque a impostare una dimensione massima del dizionario a 25'000 parole.\n",
    "Cosa facciamo con le parole che verranno tolte dal dizionario? Le andremo a rimpiazzare cone un token apposito *UNK* che sta per unknown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "controlliamo la dimensione del dizionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perchè la dimensione del dizionario è 25002? Perchè abbiamo due token aggiuntivi ```<unk>``` per gestire le parole scartate e ```<pad>```.\n",
    "Cosa serve ```<pad>``` ? \n",
    "\n",
    "Quando alimentiamo un modello a volte abbiamo la necessità che tutte le frasi abbiano la stessa lunghezza, perciò le frasi più brevi vengono riempite con ```<pad>``` affinchè tutte arrivino alla stessa lunghezza.\n",
    "\n",
    "<img src=\"./images/pad.png\" />\n",
    "\n",
    "Andiamo ad analizzare le parole più frequenti all'interno del dataset di train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 203562), (',', 192482), ('.', 165200), ('and', 109442), ('a', 109116), ('of', 100702), ('to', 93766), ('is', 76327), ('in', 61254), ('I', 54000), ('it', 53502), ('that', 49185), ('\"', 44277), (\"'s\", 43315), ('this', 42438), ('-', 36691), ('/><br', 35752), ('was', 35033), ('as', 30384), ('with', 29774)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "possiamo accedere al vocabolario direttamente usando i metodi itos (**i**nteger **to** **s**tring)  e stoi (**s**tring **to** **i**nteger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13097"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il passo finale della preparazione dei dati è creare gli iteratori. Utilizzaremo questi per navigare i dati nel ciclo training/evaluation.\n",
    "Gli iteratori ritornano un batch di esempi (già convertiti in tensori) ad ogni iterazione. Utilizzeremo un ```BucketIterator```, questo è uno speciale iteratore che ritorna i Batch di esempi di lunghezze simili, minimizzando il numero di padding per ogni iterazione. \n",
    "\n",
    "Vogliamo anche piazzare i tensori ritornati sulla GPU se possibile. E' possibile farlo andando ad impostare un ```torch.device``` all'iteratore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruzione del modello\n",
    "----\n",
    "\n",
    "Costruiamo un modello giocattolo per fare i nostri esperimenti.\n",
    "Prima di tutto bisogna creare una classe che estende ```nn.Module```. Dentro ```__init__``` si richiama con super il costruttore di ```nn.Module``` e si definiscono i layer del modello.\n",
    "Iniziamo creando tre layer:\n",
    "\n",
    "**Embedding Layer** usato per trasformare il nostro vettore one-hot in un vettore denso. Questo livello è \"semplicemente\" un Layer Fully connected. Come beneficio abbiamo anche che le parole con significato simile sono mappate vicine in questo spazio dimensionale.\n",
    "\n",
    "**RNN** La rete RNN prende in ingresso il dense vector e la memoria precedente $h_{(t-1)}$ e la usa per calcolare $h_t$\n",
    "\n",
    "<img src=\"./images/rnn.png\" />\n",
    "\n",
    "\n",
    "**Linear Layer** Alla fine il linear layer prende lo hidden state finale e lo usa per produrre l'output $f(h_t)$ nella dimensione voluta.\n",
    "\n",
    "Il metodo ```forward``` viene chiamato per produrre l'output partendo dagli esempi.\n",
    "\n",
    "Ogni batch, text è un tensore di dimensione **[sentence lenght,batch size]**, questo verrà trasformato in un vettore one_hot.\n",
    "La trasformazione in one_hot viene fatta al volo all'interno del modulo di embedding per risparmiare spazio.\n",
    "\n",
    "L'output viene poi passato al layer di embedding che restituisce la versione vettorializzata della frase, embedded è un tensore di dimensione **[sentence lenght,batch size,embeddind dim]**.\n",
    "\n",
    "embedded poi alimenta la RNN, Pytorch se non viene passato nessun vettore $h_0$ lo imposta in automatico con tutti i valori a 0.\n",
    "\n",
    "La RNN ritorna due tensori, ```output``` di dimensione **[sentence lenght,batch size,hidden dim]** e ```hidden``` che rappresenta l'ultimo hidden state. Output rappresenta la concatenazione di tutti gli hidden state. \n",
    "Lo verifichiamo con la assert, ```squeeze``` serve per rimuovere la dimensione 1.\n",
    "\n",
    "alla fine passiamo ```hidden``` alla rete neuronale ```fc``` per ottenere il risultato\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #la dimensione di text è [sent len, batch size]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #la dimensione di embedded è [sent len, batch size, emb dim]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output contiene tutti gli hidden state concatenati [sent len, batch size, hid dim]\n",
    "        #la dimensione hidden è [1, batch size, hid dim] contiene l'ultimo stato della RNN\n",
    "        \n",
    "        hidden_squeeze = hidden.squeeze(0)\n",
    "        assert torch.equal(output[-1,:,:], hidden_squeeze)\n",
    "        return self.fc(hidden_squeeze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "andiamo a instanziare la rete neuronale impostando l'ingresso con dimensionalità del dizionario *one-hot*, e un output di dimensione 1 (problema binario) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(25002, 100)\n",
       "  (rnn): RNN(100, 256)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creiamoci anche una funzione che ci dice il numero di parametri addestrabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train del modello\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa bisogna scegliere un ottimizzatore, questo è l'algoritmo che va ad ottimizzare i pesi della rete. \n",
    "Scegliamo un semplice **SGD stocastic gradiend descent**. Il primo sono i pesi che verranno ottimizzati, il secondo è il \n",
    "**leaning rate** che ci da la velocità con cui l'ottimizzatore cerca di avvicinarsi ad una possibile soluzione.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo ora una funzione di loss, questa funzione ci dice quanto siamo lontani dalla soluzione. In Pytorch viene spesso chiamata ```criterion``` .\n",
    "\n",
    "Useremo la funzione *binary cross entropy with logits*.\n",
    "\n",
    "Il nostro modello restituisce un numero reale (non compreso tra 0 e 1) mentre le nostre etichette sono due numeri interi 0 e 1, dobbiamo restringere il campo dell'uscita utilizzando una funzione *logit* o *sigmoide*\n",
    "\n",
    "Una volta ristretto il campo dobbiamo calcolare il loss (quanto siamo lontani dalla soluzione) utilizzando la formula dell'entropia incrociata [binary cross entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/).\n",
    "\n",
    "La funzione  ```BCEWithLogitsLoss ``` fa entrambi questi passi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ora se abbiamo abilitato una GPU spostiamo l'ottimizzatore e il criterion con ```.to()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci creiamo anche una funzione per valutare visivamente se il nostro modello sta lavorando bene (metrica), usiamo la funzione accuratezza.\n",
    "\n",
    "Attenzione non è detto che questa metrica vada bene per tutti i problemi.\n",
    "\n",
    "Prima alimentiamo la rete, poi schiacciamo il risultato tra 0 e 1 e poi lo approssimiamo al primo intero, il round fa si che se il numero è maggiore di 0.5 il risultato sarà 1 altrimenti sarà 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Ritorna l'accuratezza per ogni batch ad esempio se abbiamo indovinato 8 esempi su 10 avremo un risultato di 0.8\n",
    "    \"\"\"\n",
    "\n",
    "    #arrotondo il risultato\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    \n",
    "    #cambio il tipo in float in quanto altrimento avrei una divisione intera\n",
    "    correct = (rounded_preds == y).float()  \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamo pronti a creare la funzione di train che andrà ad analizzare tutti i record del dataset a batch con dimensione ```BATCH_SIZE```.\n",
    "\n",
    "Come prima cosa mettiamo il modello in uno stato di train con ```model.train()``` per abilitare il *dropout* e la *batch normalization*, sebbene in questo modello non ci sono è una buona pratica. \n",
    "\n",
    "Per ogni batch andiamo ad azzerare il gradiente, ogni parametro del modello ha un attributo ```grad``` che viene calcoltato con ```criterion```, questo attributo non viene ripulito da pytorch in automatico perciò dobbiamo farlo noi a mano.\n",
    "\n",
    "Andiamo poi ad alimentare il modello con il testo ```batch.text``` (attenzione che questo attributo cambia per ogni dataset).\n",
    "In automatico viene chiamata la funzione ```forward```.\n",
    "\n",
    "Come passo successivo andiamo a togliere una dimensione al risultato per allinearlo al campo ```batch.label```.\n",
    "Andiamo poi a calcolare il loss e l'accuracy del batch che verranno sommati per calcolare la media alla fine delle iterazioni.\n",
    "\n",
    "I passi più importanti sono ```loss.backward()``` che si occupa del calcolo del gradiente per ogni peso e ```optimizer.step()``` che aggiorna il modello.\n",
    "\n",
    "Se avete fatto caso abbiamo detto esplicitamente che ```LABEL ``` è un campo float, questo perchè se non lo avessimo fatto perchè TorchText in automatico mappa i campi come ```LongTensors``` invece il criterion vuole un float  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```evaluate``` è simile al train tranne per il fatto che non dobbiamo aggiornare i pesi e non dobbiamo utilizzare il *dropout* e la *batch normalization*.\n",
    "\n",
    "Non dobbiamo nemmeno calcolare il gradiente e questo lo si fa con ```no_grad()``` questo fa si che si usi meno memoria e il calcolo risulta più veloce.\n",
    "\n",
    "Il resto della funzione è uguale a train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in tqdm(iterator):\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora è giunto il momento di unire tutti i pezzi e vedere il risultato, ad ogni epoch viene fatto un train su tutti i batch e viene calcolato quanto il modello si comporta bene sul dataset di validation.\n",
    "\n",
    "Inoltre se il modello ottiene il miglior risultato di loss sul dataset di validation ne salvo i pesi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 274/274 [00:20<00:00, 13.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 75.81it/s]\n",
      "  1%|▌                                                                                 | 2/274 [00:00<00:21, 12.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.694 | Train Acc: 50.25%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 274/274 [00:19<00:00, 14.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 82.48it/s]\n",
      "  0%|▎                                                                                 | 1/274 [00:00<00:30,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.86%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 50.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 274/274 [00:19<00:00, 14.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 75.48it/s]\n",
      "  1%|▌                                                                                 | 2/274 [00:00<00:21, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.19%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 50.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 274/274 [00:19<00:00, 13.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 76.53it/s]\n",
      "  1%|▌                                                                                 | 2/274 [00:00<00:23, 11.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.83%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 49.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 274/274 [00:20<00:00, 13.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 78.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.03%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 50.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora abbiamo visto che il nostro modello non si è comportato molto bene, poco male non era questo lo scopo dell'esercizio. \n",
    "Andiamo a vedere come il modello si comporta sul dataset di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 391/391 [00:05<00:00, 77.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.709 | Test Acc: 47.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo anche a provare il modello su una frase a nostro piacimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un esempio di recensione negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.444013386964798"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un esempio di recensione positiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48224857449531555"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

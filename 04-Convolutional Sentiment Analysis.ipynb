{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Sentiment Analysis\n",
    "====\n",
    "\n",
    "Tradizionalmente le reti convoluzionali sono usate per analizzare le immagini, i layer di convoluzione di solito sono seguiti da uno o piu linear layer. I layer convoluzionali usano filtri (chiamati anche kernel) che scansionano l'immagine e ne creano un'altra. L'idea intuitiva che sta dietro all'apprendimento delle reti convoluzionali è che lavorano come estrattori di feature. Andandosi a concentrare sulle parti più importanti della nostra immagine.\n",
    "\n",
    "Come si usano le reti convoluzionali sul testo? Ad esempio un filtro 1x2 può controllare due parole sequenziali, bi-gram. \n",
    "L'intuizione è che la presenza di alcuni bi-grams o tri-grams in una frase sono un buon indicatore del risultato finale.\n",
    "\n",
    "Preparazione dei dati\n",
    "----\n",
    "\n",
    "Invece di creare i bi-grams come nel modello FastText lasceremo che sia lo strato di convoluzione a fare questo lavoro.\n",
    "Il layer di convoluzione si aspetta che la dimensione del batch sia la prima, dobbiamo dire a TorchText di preparare i dati in questo modo andando ad esplicitarlo con il parametro batch_first = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', batch_first = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo il vocabolario e carichiamo il vettore di word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come prima creiamo gli iteratori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costuiamo il modello\n",
    "----\n",
    "\n",
    "Andiamo a vedere come costruire una CNSS da usare per il testo. Le immagini sono tipicamente bidimensionali (non consideriamo la dimensione dei colori) mentre il testo viene trasformato in una sequenza di numeri (monodimensionale).\n",
    "\n",
    "Però sappiamo che il primo passo di quasi tutti i notebook precedentei è stato convertire le parole in word embeddings.\n",
    "Ecco come possiamo immaginare le parole nella seconda dimensione, ogni parola lungo un asse e gli elementi del vettore lungo l'altra. Analizziamo la rappresentazione a due dimensioni della frase seguente:\n",
    "\n",
    "<img src=\"./images/conv.png\">\n",
    "\n",
    "Possiamo usare un filtro di dimensione [n x emb_dim]. Questo coprirà $n$ parole in sequenza\n",
    "Considera l'ìmmagine qui sotto, con i nostri word vectori rappresentati in verde. Abbiamo 4 parole con una dimensionalità di embedding impostata a 5, creiamo dunque un tensore \"immagine\" [4x5]\n",
    "\n",
    "Un filtro che copre due parole alla volta dovrà essere un filtro [2x5], mostrato in giallo.\n",
    "L'output del filtro, in rosso sarà un singolo numero risultato della convoluzione.\n",
    "\n",
    "<img src=\"./images/conv1.png\">\n",
    "\n",
    "il filtro si muove in basso e calcola il prossimo risultato della convoluzione\n",
    "\n",
    "<img src=\"./images/conv2.png\">\n",
    "\n",
    "Fino alla fine della frase\n",
    "\n",
    "Nel nostro caso avremo come risultato un vettore con il numero di elementi pari alla lughezza della frase meno l'altezza del filtro più uno nel nostro caso $4-2+1=3$.\n",
    "\n",
    "\n",
    "L'esempio mostra come calcolare l'output con un solo filtro. Il nostro modello, avrà molti di questi filtri. L'idea è che ogni filtro si concentrerà su una differente feature da estrarre. \n",
    "\n",
    "Nel nostro modello avremo anche differenti dimensioni dei filtri, con dimensione 3,4 e 5 con centinaia di questi. L'intuizione è che guarderemo differenti occorrenze di tri-grams, 4-grams and 5-grams che sono rilevanti per l'analisi del sentiment delle recensioni dei nostri film.\n",
    "\n",
    "Il posso successivo del modello e usare il pooling (max pooling) sull'ouptut del layer convoluzionale.\n",
    "Questo è simile a quanto fatto nel modello FastText dove andavamo a calcolare la media di ogni word vector, con la funzione F.avg_pool2d, ora invece di calcolare la media su una dimensione, andremo a prendere il valore massimo.\n",
    "Qui sotto un esempio grafico.\n",
    "\n",
    "<img src=\"./images/max_pooling.png\">\n",
    "\n",
    "L'idea è che il valore massimo è la feature \"più importante\" per determinare il sentiment di una recensione, che corrisponde al n-gram \"più importante\" della recensione.\n",
    "\n",
    "Come facciamo a riconoscere l'n-gram più importante? Fortunatamente non dobbiamo farlo noi!. Tramite la backpropagation, i pesi dei filtri sono cambiati in modo da far risultare certi n-gram più indicativi nella recensione che abbiamo letto e dargli un valore più alto.\n",
    "\n",
    "Il nostro modello ha 100 filtri con 3 differenti dimensioni, questo significa che si concentrerà su 300 differenti n-grams.\n",
    "Concateneremo i risultato di questi filtri in un singolo vettore e lo passeremo ad un linear layer per ottenere il risultato.\n",
    "\n",
    "Possiamo pensare ai pesi dell'ultimo livello come un \"soppesatore del risultato\" per ognino dei 300 n-grams per ottenere il risultato finale.\n",
    "\n",
    "Implementazione nel dettaglio\n",
    "----\n",
    "Implementeremo i layer convoluzionali con la funzione nn.Conv2d. Il parametro in_channels è il numero dei canali nella nostra immagine nel layer convoluzionale. Le immagini di solito ne hanno 3 (il canale rosso,blu e verde), stiamo usando del testo e dunque avremo un canale soltanto. `out_channels` è il numero dei filtri mentre `kernel_size` è la dimensione dei filtri stessi.\n",
    "Ogni `kernel_sizes` avrà una dimensione [n x emb_dim] dove $n$ è la dimensione dei n-grams.\n",
    "\n",
    "In PyTorch, le RNN vogliono la dimensione del batch in imput come seconda dimensione, mentre le CNN volgiono la dimensione del batch come prima, non dobbiamo cambiare niente se abbiamo già impostato `batch_first = True` in campo TEXT.\n",
    "Possiamo poi passare la frase nel nostra layer di embedding. La seconda dimensione del nostro input è il numero di canali da dare alla funzione `nn.Conv2d`. Un testo tecnicamente non a la dimensione channel, eseguiamo un `unsqueeze` del nostro tensore per crearne una.\n",
    "\n",
    "Passiamo dunque i tensori lungo i layer convoluzionali e il pooling, usiamo anche la activation function RelU dopo ogni convolutional layer.\n",
    "Un'altra simpatica feature del pooling layer è che si può lavorare frasi con lunghezze differenti.\n",
    "La dimensione dell'uscita del layer convoluzionale è solo dipendente dal numero di filtri. Senza il layer max pooling l'ingresso del layer linear dipenderebbe dalla dimensione della frase in input (e non è quello che si vuole).\n",
    "Una opzione sarebbe quella di tagliare o riempire tutte le frasi per averle tutte uguali, comunque con il layer max pooling siamo sicuri che il linear layer sarà sempre ad una dimensione fissa.\n",
    "\n",
    "**Nota:** Otteremo una eccezione se la nostra frase sarà più corta del più grande filtro utilizzato. Se questo dovesse succedere dobbiamo usare i token `<pad>` per riempire la frase. Comunque nell' IMDb non ci sono frasi più corte di 5 parole perciò possiamo proseguire tranquilli.\n",
    "\n",
    "Alla fine eseguiamo un dropout sulla concatenazione dei filtri e diamo il tensore il tensore al linear layer per ottenere il risultato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[0], embedding_dim))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[1], embedding_dim))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[2], embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currentemente il modello CNN può usare solo 3 differenti dimensioni di filtri, ma possiamo milgiorare il codice del nostro modello e rendelo più generico e prendere ogni numero di filtri.\n",
    "\n",
    "Possiamo fare questo mettendo tutti i nostri filtri convoluzionali in un `nn.ModuleList`, una funzione di PyTorch per gestire una lista di `nn.Modules`.\n",
    "\n",
    "Se aggiungessimo semplicemente una lista Pyhton, i moduli nella lista non verrebbero \"visti\" da PyTorch e questo ci darebbe dei problemi. \n",
    "\n",
    "Ora possiamo usare una lista arbitraria di dimensioni di filtri, nella parte di codice che esegue la list comprehension creeremo i convolutional layer per ognuno dei filtri richiesti. \n",
    "\n",
    "Nel metodo forward passiamo ogni elemento nella lista convolutional layer e lo applichiamo alla frase in ingresso, al risultato applichiamo il max pool prima di concatenare il risultato e passarlo prima al dropout e poi al linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo anche implementare il modello sopra usando dei layers 1-dimensional convolutional, dove la dimensione di embedding è la profondità del filtro e il numero dei token è il parametro width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1d(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv1d(in_channels = embedding_dim, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = fs)\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        #embedded = [batch size, emb dim, sent len]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        \n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "andiamo ad instanziare il modello "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "come sempre fatto andiamo a vedere quanti parametri ha il modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,600,801 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a caricare il vettore di embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.1386,  0.1180,  0.3534,  ...,  0.1226,  0.5973, -0.1702],\n",
       "        [-0.0786,  0.0541, -0.0993,  ...,  0.2565, -0.1874, -0.4428],\n",
       "        [-0.3617,  0.6201,  0.1105,  ...,  0.2994, -0.5920,  1.0949]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inizializziamo poi il vettore di embedding unk e pad a zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train del modello\n",
    "---\n",
    "\n",
    "La fase di train del modello non cambia dai precedenti notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.124 | Train Acc: 95.53%\n",
      "\t Val. Loss: 0.346 |  Val. Acc: 86.50%\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.085 | Train Acc: 97.01%\n",
      "\t Val. Loss: 0.351 |  Val. Acc: 87.20%\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.063 | Train Acc: 98.05%\n",
      "\t Val. Loss: 0.380 |  Val. Acc: 86.79%\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.56%\n",
      "\t Val. Loss: 0.415 |  Val. Acc: 87.19%\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.033 | Train Acc: 99.00%\n",
      "\t Val. Loss: 0.437 |  Val. Acc: 87.16%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.399 | Test Acc: 84.86%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input personalizzato\n",
    "---\n",
    "\n",
    "**Nota:** Come scritto prima, se la frase di input è più breve del filtro più grande avremo un errore. Per evitare questo la nostra funzione `predict_sentiment` accetta iun parametro `min_len`.\n",
    "\n",
    "Se la frase in ingresso ha meno token di min_len, andremo a riempire la frase con i tag di padding fino a raggiungere quota min_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con una frase negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07555589079856873"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con una frase positiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9691400527954102"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
